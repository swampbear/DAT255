{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmtchwQ3syQ6"
      },
      "source": [
        "# Data loading in Keras and TensorFlow\n",
        "\n",
        "Modern deep learning comes with two considerations that affect the way we process input data:\n",
        "\n",
        "1. The data are typically too big to fit in memory.\n",
        "2. We usually have two separate computing devices, the CPU and the GPU.\n",
        "\n",
        "Point 1 means that we have to process the data in _batches_, where we load a subset of the data into memory, run one step of gradient descent on it, and then load the next subset and proceed with another training step.\n",
        "\n",
        "Point 2 means that while the GPU is running gradient descent and backpropagation on one batch, the GPU is free to load and pre-process the next batch in parallel. So whenever the GPU is done with one training iteration, it  can immediately start with the next one, without waiting for the data to be loaded from disk.\n",
        "\n",
        "TensorFlow provides the functionality to do this efficiently without requiring much effort from us, and in this notebook we will try it out for different types of input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB2RHR_xw8e7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux00D7rvxP4D"
      },
      "source": [
        "The core object we will interact with is a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), which has useful methods like\n",
        "\n",
        " - `batch(batch_size)` which makes batches of give size,\n",
        " - `prefetch(buffer_size)` which load the next batch in advance, and\n",
        " - `map(map_func)` which applies a function to each element, like Python's built-in `map()` function.\n",
        "\n",
        " For additional information, have a look at the `tf.data` [tutorial](https://www.tensorflow.org/guide/data), and the `tf.data.Dataset` [documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNyz4FXSzJle"
      },
      "source": [
        "First, let's make a `Dataset` from a list or an array, which is done by a function with the very non-obvious name `from_tensor_slices`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6P9sDAezIWZ"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c8xZp0_1MqL"
      },
      "source": [
        "The contents of a `Dataset` is more or less hidden to us, since its elements usually doesn't exist before they are needed. But, they are iterable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1nRMRAZ14SX"
      },
      "outputs": [],
      "source": [
        "for element in dataset:\n",
        "    print(element)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VjzXOsG2F0g"
      },
      "source": [
        "The elements are `tf.Tensor`s, with their benefits and drawbacks, but remember we can convert to regular NumPy arrays by calling `.numpy()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6pQkHJX2ok5"
      },
      "source": [
        "### <span style=\"color: red; font-weight: bold;\">Exercise:<span>\n",
        "\n",
        "From the above `dataset`, extract the original Python list of integers, `[8, 3, 0, 8, 2, 1]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-MHQYLl27ct"
      },
      "outputs": [],
      "source": [
        "original_list = ...\n",
        "print(original_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF-5_JNY3ZXI"
      },
      "source": [
        "## Reading data\n",
        "\n",
        "Let's start doing more realistic stuff, like reading in different types of file formats.\n",
        "\n",
        "We will try CSV files, images, and a custom file format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L2E8J5I5Qil"
      },
      "source": [
        "### CSV files\n",
        "\n",
        "Columns of data stored in CSV (_comma-separated values_) files can be imported to TensorFlow through the common [Pandas](https://pandas.pydata.org/docs/index.html) format, which is shown in this [tutorial](https://www.tensorflow.org/tutorials/load_data/csv). But in case all the data fits in memory, there is not that much of a reason to use `tf.data.Dataset` in the first place.\n",
        "\n",
        "Let's construct an example where we have data spread over many CSV files, and want to read them in in an efficient manner.\n",
        "\n",
        "Here we get the _California housing prices_ dataset, and simply split it in 20 different files, for illustration purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1jsCAaf64k2"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def save_to_csv_files(data, name_prefix, header=None, n_parts=10):\n",
        "    housing_dir = Path() / \"datasets\" / \"housing\"\n",
        "    housing_dir.mkdir(parents=True, exist_ok=True)\n",
        "    filename_format = \"my_{}_{:02d}.csv\"\n",
        "\n",
        "    filepaths = []\n",
        "    m = len(data)\n",
        "    chunks = np.array_split(np.arange(m), n_parts)\n",
        "    for file_idx, row_indices in enumerate(chunks):\n",
        "        part_csv = housing_dir / filename_format.format(name_prefix, file_idx)\n",
        "        filepaths.append(str(part_csv))\n",
        "        with open(part_csv, \"w\") as f:\n",
        "            if header is not None:\n",
        "                f.write(header)\n",
        "                f.write(\"\\n\")\n",
        "            for row_idx in row_indices:\n",
        "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
        "                f.write(\"\\n\")\n",
        "    return filepaths\n",
        "\n",
        "train_data = np.c_[X_train, y_train]\n",
        "valid_data = np.c_[X_valid, y_valid]\n",
        "test_data = np.c_[X_test, y_test]\n",
        "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
        "header = \",\".join(header_cols)\n",
        "\n",
        "train_filepaths = save_to_csv_files(train_data, \"train\", header, n_parts=20)\n",
        "valid_filepaths = save_to_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
        "test_filepaths = save_to_csv_files(test_data, \"test\", header, n_parts=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUv6l_1EhCox"
      },
      "source": [
        "One such file now looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0JEfJZnhE-e"
      },
      "outputs": [],
      "source": [
        "print(\"\".join(open(train_filepaths[0]).readlines()[:4]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUYtgyJKhIHZ"
      },
      "source": [
        "and we have these different file paths:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L31Nox4XhK0l"
      },
      "outputs": [],
      "source": [
        "print(train_filepaths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIArbAjmhP8V"
      },
      "source": [
        "### Pipeline for reading multiple files\n",
        "\n",
        "Now we start building out input pipeline. This can be done for any type of files (not just CSV), but we show it here for the CSV case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC8aShihhjVN"
      },
      "outputs": [],
      "source": [
        "# A dataset containing our list of files.\n",
        "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
        "\n",
        "# (check that it works as expected)\n",
        "for filepath in filepath_dataset:\n",
        "    print(filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh1cDPmohzqy"
      },
      "source": [
        "We would like the files to be read in parallel, so we can process the contents of one while loading another.\n",
        "\n",
        "This is achieved by _interleaving_ the files, using `tf.data.Dataset.interleave()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzEHKl87iX1h"
      },
      "outputs": [],
      "source": [
        "# Number of parallel processes. This can also be set to `tf.data.AUTOTUNE`, then\n",
        "# TensorFlow determines the value itself.\n",
        "n_readers = 5\n",
        "\n",
        "# Now make a new dataset from the file paths.\n",
        "# We use `TextLineDataset` since we have text inputs,\n",
        "# and `skip(1)` skips the header line.\n",
        "dataset = filepath_dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "    cycle_length=n_readers\n",
        ")\n",
        "\n",
        "# Print the first five values:\n",
        "for line in dataset.take(5):\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFpn6HpTjPtF"
      },
      "source": [
        "### Add preprocessing\n",
        "\n",
        "From the above cell we see that our values are still strings, but we want floats, and probably some additional steps too, like standardisation of features.\n",
        "\n",
        "Let's add a preprocessing function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4YCkHXFkWhA"
      },
      "source": [
        "### <span style=\"color: red; font-weight: bold;\">Exercise:<span>\n",
        "\n",
        "implement feature standardisation (you may use scikit-learn's StandardScaler) in the `preprocess` function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDZa97EWjsrt"
      },
      "outputs": [],
      "source": [
        "num_columns = 8\n",
        "\n",
        "def parse_csv_line(line):\n",
        "    \"\"\"\n",
        "    TensorFlow is peculiar about its types, have a look\n",
        "    at https://www.tensorflow.org/api_docs/python/tf/io/decode_csv\n",
        "    \"\"\"\n",
        "    defs = [float()] * num_columns\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "\n",
        "    # Return first the features, then the target\n",
        "    return tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
        "\n",
        "\n",
        "def preprocess(line):\n",
        "    x, y = parse_csv_line(line)\n",
        "\n",
        "    # TODO:\n",
        "    # Feature standardisation\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di7eC8sokv4A"
      },
      "source": [
        "### Add together everything\n",
        "\n",
        "Now we apply the preprocessing function to our dataset, and do the remaining performance steps: batching and pre-fetching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OColG0e2k3jK"
      },
      "outputs": [],
      "source": [
        "preprocesse_dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "shuffled_dataset = dataset.shuffle(buffer_size=10000)\n",
        "batched_dataset = dataset.batch(batch_size=128)\n",
        "prefetched_dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69FzC15qlfx2"
      },
      "source": [
        "Like always, see if it works!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0dmPNf2lj73"
      },
      "source": [
        "### <span style=\"color: red; font-weight: bold;\">Exercise:<span>\n",
        "\n",
        "Print the first three data points of the final dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s81pRK_-lvNh"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B95bFGftl1h8"
      },
      "source": [
        "## Images\n",
        "\n",
        "For images we typically always store one image in one file, and Keras gives us a very nice conventience function for getting images into a `tf.data.Dataset`, `keras.utils.image_dataset_from_directory()`, which we have used in previous notebooks already.\n",
        "\n",
        "The requirements for using it is that we save images in a directory structure that looks like\n",
        "```\n",
        "main_directory/\n",
        "...class_a/\n",
        "......a_image_1.jpg\n",
        "......a_image_2.jpg\n",
        "...class_b/\n",
        "......b_image_1.jpg\n",
        "......b_image_2.jpg\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cza5PXEm_xK"
      },
      "source": [
        "Even though we are already familiar with it, let's give it a little test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3VjwYnXnGmj"
      },
      "outputs": [],
      "source": [
        "!curl -O https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\n",
        "!unzip -q kagglecatsanddogs_5340.zip\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvbcuuE-nVJZ"
      },
      "source": [
        "EXERCISE\n",
        "\n",
        "Load the downloaded images into a dataset using `keras.utils.image_dataset_from_directory()`, satisfying the following conditions:\n",
        "- images have 124x124 pixels resolution\n",
        "- labels are categorical\n",
        "- the batch size is 64\n",
        "- the image order is shuffled.\n",
        "\n",
        "Then plot the first three images in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qnaB2mgoEW8"
      },
      "source": [
        "## Custom data\n",
        "\n",
        "In case you need to load data of a custom format -- which often happens in research and development settings -- the step of getting a single file or data point into a `tf.Tensor` has to be specifically coded, but going from `tf.Tensor` to a `tf.data.Dataset` is still rather general.\n",
        "\n",
        "Let's make a silly example just to illustrate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_PCEkJdp2H3"
      },
      "outputs": [],
      "source": [
        "# Some files in a wacky binary format, which contains an unknown number of data\n",
        "# points each.\n",
        "my_files = [\n",
        "    'file1.xyz',\n",
        "    'file2.xyz',\n",
        "    'file3.xyz',\n",
        "    'file4.xyz',\n",
        "    'file5.xyz'\n",
        "]\n",
        "\n",
        "# (just write empty files)\n",
        "for filename in my_files:\n",
        "    with open(filename, 'w') as fout:\n",
        "        fout.write('xyz')\n",
        "\n",
        "def read_file(filename, num_columns=10):\n",
        "    \"\"\"\n",
        "    Here we just generate some random stuff :)\n",
        "    \"\"\"\n",
        "\n",
        "    # Random number of data points\n",
        "    num_data_points = np.random.randint(1, 10)\n",
        "\n",
        "    for _ in range(num_data_points):\n",
        "\n",
        "        # Random data\n",
        "        data = np.random.uniform(size=(num_columns,))\n",
        "\n",
        "        # Convert to Tensor\n",
        "        data = tf.constant(data)\n",
        "\n",
        "        # Return one data point at a time.\n",
        "        yield data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymnTwRgErifq"
      },
      "source": [
        "With our custom file reading function, we can use `tf.dataset.Dataset.from_generator`, which doesn't need to know beforehand how many data points each file contains. It **does** need to know, however, the shape/length of each data point, which is specified as `output_signature`.\n",
        "\n",
        "Writing custom functions with `tf.data.Dataset` gets convoluted rather fast, but that is the price to pay for performance 🤷‍♂️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da1CNL2cryB9"
      },
      "outputs": [],
      "source": [
        "num_features = 10\n",
        "\n",
        "filepaths = tf.data.Dataset.list_files(my_files)\n",
        "\n",
        "dataset = filepaths.interleave(\n",
        "    lambda filepath: tf.data.Dataset.from_generator(\n",
        "        read_file,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(num_features, ))\n",
        "        ),\n",
        "        args=(filepath, num_features)\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR9INZkntZ04"
      },
      "source": [
        "### <span style=\"color: red; font-weight: bold;\">Exercise:<span>\n",
        "\n",
        "Batch the dataset and read the first five data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzoU38QstWog"
      },
      "outputs": [],
      "source": [
        "# Your code\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
