{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ywKCd03QJqH"
      },
      "source": [
        "# Text classification\n",
        "\n",
        "In this notebook we train networks to do _sentiment analysis_, where we will classify film reviews as either positive or negative.\n",
        "\n",
        "A popular dataset for this type of task is the [IMDb movie review](https://huggingface.co/datasets/stanfordnlp/imdb) dataset, which is described in detail in this [article](https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf). It contains in total 50 000 film reviews from the IMDb website, where users write short reviews accompanied by a rating. On basis of the rating, reviews are categorised into positive or negative, and it is our job to match the text to the class. The data are split 50/50 into 25 000 training examples, and 25 000 testing examples.\n",
        "\n",
        "We will try out a set of different models -- your regular `Dense` feed-forward network, a CNN, and an RNN. Crucial to training any model at all, is of course to convert the text into numbers. This is the **text vectorisation** step, which we will test out different approaches for here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjjkcIp0Rabj"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDcRI8oVP9mL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ztf7svfZRcSi"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1xmSqlHRjl9"
      },
      "outputs": [],
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\n",
        "    \"aclImdb_v1\",\n",
        "    url,\n",
        "    untar=True,\n",
        "    cache_dir='.',\n",
        "    cache_subdir=''\n",
        ")\n",
        "\n",
        "dataset_dir = 'aclImdb_v1/aclImdb'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8VAB87dRmbl"
      },
      "source": [
        "Let's list the contents of the dataset directory. It contains one directory with positive reviews (`pos`), one with negative (`neg`), and one for unsupervised learning (`unsup`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZpTat0yRsiX",
        "outputId": "4cce8215-bc5b-4ea9-c43c-4a2f5d8cb76d"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCmF60lzRwAH"
      },
      "source": [
        "We will not use the set for unsupervised learning, so this we can delete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "Qon8DcaQR04i",
        "outputId": "13555193-b83a-45f0-84d4-98363faafb73"
      },
      "outputs": [],
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPIHupn1R3ls"
      },
      "source": [
        "## Create TensorFlow datasets\n",
        "\n",
        "The movie reviews are stored as separate files in the `pos` and `neg` directories, and can be read into TensorFlow datasets very easily by using Keras' convenience functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz8FEYfGR3O9",
        "outputId": "a4b1c33c-cfc1-489f-dbee-a55f80fda5d4"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    os.path.join(dataset_dir, 'train'),\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aASLNAgmR7g1"
      },
      "source": [
        "Let's have a look at the first three reviews:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEcaarlTR8UE",
        "outputId": "4d0c9df9-4b47-49ae-e894-ff017db88960"
      },
      "outputs": [],
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(\"Review\", text_batch.numpy()[i])\n",
        "    print(\"Label\", label_batch.numpy()[i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjXERIwuR_30"
      },
      "source": [
        "Verify the mapping between label and class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7DKTaYHSCSx",
        "outputId": "c8b8cb9a-a51f-4d82-ae93-7725cc723ca9"
      },
      "outputs": [],
      "source": [
        "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
        "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9buAHhOSENo"
      },
      "source": [
        "Now we do the same for validation and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7en-AvKSFhJ",
        "outputId": "b86d3e05-8f46-438b-b16b-2d3d560ec055"
      },
      "outputs": [],
      "source": [
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    os.path.join(dataset_dir, 'train'),\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    os.path.join(dataset_dir, 'test'),\n",
        "    batch_size=batch_size\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCw7JgQVSI4o"
      },
      "source": [
        "## Text vectorisation\n",
        "\n",
        "Now for the essential part: Getting our text into useful numbers. For this we will use the [`TextVectorization`](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/) layer.\n",
        "\n",
        "Our text is, however, not entirely ready for processing yet. if you look at the example reviews, they contain certain HTML tags (like `<br /><br />`), and potentially problematic punctuation. We need to write a function that will _standardise_ the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYazlOJrSMd_"
      },
      "source": [
        "### <span style=\"color: red;\">Exercise:<span>\n",
        "\n",
        "Complete the function below. If should\n",
        "- Convert all charachters to lower-case\n",
        "- Remove `<br>` tags and similar\n",
        "- Remove commas, periods, and other punctuation\n",
        "\n",
        "For hints, have a look at the lecture notes. You may find the functions `tf.strings.lower` and `tf.strings.regex_replace` useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JxgYNK4SIFS"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = ...\n",
        "  without_html = ...\n",
        "  without_punctuation = ...\n",
        "  return without_punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcUjjUEPST0O"
      },
      "source": [
        "Now we can give our standardisation function as input to the `TextVectorization` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW6ioL8WSXNa"
      },
      "source": [
        "## Vectorisation approach number 1: Text as a _set_\n",
        "\n",
        "The words in a text obviously has an ordering, but is this really relevant to out classification task?\n",
        "\n",
        "For our first text vectorisation approach, we just encode the presence of words in a text, and not the ordering. We'll perform _multi-hot_ encoding, meaning that for an input text, we output a vector where each column represents a word in the vocabulary, and 1 indicatres the words is present while 0 means it is not.\n",
        "\n",
        "Consider this little eaxmple of multi-hot text vectorisation:\n",
        "\n",
        "```{python}\n",
        ">>> l = keras.layers.TextVectorization(output_mode=\"multi_hot\")\n",
        ">>> l.adapt([\"the cat sat on the mat\"])\n",
        ">>> l.get_vocabulary()\n",
        "['[UNK]', np.str_('the'), np.str_('sat'), np.str_('on'), np.str_('mat'), np.str_('cat')]\n",
        ">>> l([\"the cat sat on the mat\"])\n",
        "<tf.Tensor: shape=(1, 6), dtype=int64, numpy=array([[0, 1, 1, 1, 1, 1]])>\n",
        "```\n",
        "\n",
        "In terms of the N-gram naming, this would be _unigrams_, as we are looking at word sequences of length one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNldhSbLXKze"
      },
      "source": [
        "Let's implement the `TextVectorization` layer on our actual data, using also our `custom_standardization` function.\n",
        "\n",
        "It is generally a good idea to limit the size of the vocabulary to some tens of thousants of words. While modern LLMs have a vocabulary size from 40k to over 100k, we can do well with 10-20k or less for our classification task. Technically, LLMs have vocabularies that consist of _subwords_, so it doesn't really compare in any case. Anyway, let's also pad the output so we always get the same output length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sldRMNPXedK"
      },
      "outputs": [],
      "source": [
        "max_features = 10000\n",
        "\n",
        "multihot_vectorize_layer = keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='multi_hot',\n",
        "    pad_to_max_tokens=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAI_QKYMSlzi"
      },
      "source": [
        "The vectorisation layer can (as we have seen with other preprocessing layers before) be used as part of a model, or applied directly to the dataset.\n",
        "Let's choose the second option this time, so we can investigate the effect of the vectorisation. Technically speaking, this is also brings a performance benefit since in is done asynchronously on the CPU during training.\n",
        "\n",
        "First, call `adapt()` to learn the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLz49MgLZlRe"
      },
      "outputs": [],
      "source": [
        "# Extract only the data (and not the labels)\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "\n",
        "# Adapt\n",
        "multihot_vectorize_layer.adapt(train_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEs1tw50Z4r0"
      },
      "source": [
        "We can look at the vocabulary by calling `get_vocabulary()`. Print the ten first entries, along with their indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UQgwUAJZ6oW",
        "outputId": "0a187c27-6b07-4d14-e185-0b997fa8c716"
      },
      "outputs": [],
      "source": [
        "ten_first = multihot_vectorize_layer.get_vocabulary()[:10]\n",
        "\n",
        "print('index    token')\n",
        "for i, v in enumerate(ten_first):\n",
        "    print(f'{i} \\t \\'{v}\\'')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY1NIICYaqw_"
      },
      "source": [
        "Give it a test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFLEkC27aU2T",
        "outputId": "287965e9-239c-4e4b-d0f4-d5967726416d"
      },
      "outputs": [],
      "source": [
        "multihot_vectorize_layer([\"and this is a test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ5MMolUawvg"
      },
      "source": [
        "Great, it works -- now we can apply it to our TensorFlow datasets, and do caching + prefetching, for better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRA1Nsgxa6Yl"
      },
      "outputs": [],
      "source": [
        "def multihot_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return multihot_vectorize_layer(text), label\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "multihot_train_ds = raw_train_ds.map(multihot_vectorize_text)\n",
        "multihot_train_ds = multihot_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "multihot_val_ds = raw_val_ds.map(multihot_vectorize_text)\n",
        "multihot_val_ds = multihot_val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "multihot_test_ds = raw_test_ds.map(multihot_vectorize_text)\n",
        "multihot_test_ds = multihot_test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnzTgVdGbN7P"
      },
      "source": [
        "### Train a densely-connected model\n",
        "\n",
        "Having vectorised all our text into an unordered set, it is time to train the first (simple) model: A densely connected network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "PlMtBorfb03a",
        "outputId": "d3c998e5-ef3f-467a-9e90-b7ec1de7504b"
      },
      "outputs": [],
      "source": [
        "dense_model = tf.keras.Sequential([\n",
        "  keras.Input(shape=(max_features,)),\n",
        "  keras.layers.Dense(64, activation='relu'),\n",
        "  keras.layers.Dropout(0.4),\n",
        "  keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "dense_model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['binary_accuracy']\n",
        ")\n",
        "dense_model.summary()\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "history = dense_model.fit(\n",
        "    multihot_train_ds,\n",
        "    validation_data=multihot_val_ds,\n",
        "    epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glWN3gVXb9LQ"
      },
      "source": [
        "Evaluate on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIs03rnOb-18",
        "outputId": "27b38313-3d20-4fa4-d279-48ae8ad532b9"
      },
      "outputs": [],
      "source": [
        "print('Loss and accuracy on the test set:')\n",
        "dense_model.evaluate(multihot_test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii0P7Atdke0a"
      },
      "source": [
        "Let's test the model on our own film reviews!\n",
        "\n",
        "To include the text vectorization layer, whihc we now did ourside of the Keras model, we can create a new model object that incorporates it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtWJX_Atkc8b"
      },
      "outputs": [],
      "source": [
        "model_with_vectorisation = tf.keras.Sequential([\n",
        "    multihot_vectorize_layer,\n",
        "    dense_model,\n",
        "])\n",
        "\n",
        "model_with_vectorisation.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['binary_accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J4xRlLqlCFQ",
        "outputId": "4818a962-33de-4e8b-b43b-234c6ae3faf6"
      },
      "outputs": [],
      "source": [
        "examples = tf.constant([\n",
        "  \"The movie was great!\",\n",
        "  \"The movie was okay.\",\n",
        "  \"The movie was terrible...\"\n",
        "])\n",
        "\n",
        "model_with_vectorisation.predict(examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrhxlvvvcbAE"
      },
      "source": [
        "### <span style=\"color: red;\">Optional exercise:<span>\n",
        "\n",
        "Train a **bigram** model, but vectorising the text into word pairs using\n",
        "\n",
        "```{python}\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KL-V2HSVdlJQ"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmFPMM1CdpRO"
      },
      "source": [
        "## Vectorisation approach number 2: Text as a _sequence_\n",
        "\n",
        "Dropping the word ordering seems a bit unnatural, so let's keep it, and move to model types that can operate on ordered sequences instead.\n",
        "\n",
        "Then we need a different word encoding scheme -- we code the words to integers instead. The changes required to the `TextVectorization` layer are small:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e03GLvdfQp2"
      },
      "outputs": [],
      "source": [
        "sequence_length = 300   # cut the text if longer than this\n",
        "\n",
        "integer_vectorize_layer = keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9TJ7lk8fp2S"
      },
      "source": [
        "Again, adapt it to the data, and have a look at the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb2SNQe8fpXw",
        "outputId": "e7128aa7-eba4-47ed-b1eb-c1743f408516"
      },
      "outputs": [],
      "source": [
        "integer_vectorize_layer.adapt(train_text)\n",
        "\n",
        "ten_first = integer_vectorize_layer.get_vocabulary()[:10]\n",
        "\n",
        "print('index    token')\n",
        "for i, v in enumerate(ten_first):\n",
        "    print(f'{i} \\t \\'{v}\\'')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQTUlGdff8kI"
      },
      "source": [
        "Notice that we got an extra token at the beginning --  index 0, which maps to nothing, which we can interpret as \"not a word\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47qyVIDQgThM"
      },
      "source": [
        "Let's vectorise one review text, and have a look at the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxUwhrL7gVNS",
        "outputId": "76b3d654-8b33-49dd-d4e9-0d1d656b1857"
      },
      "outputs": [],
      "source": [
        "def integer_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return integer_vectorize_layer(text), label\n",
        "\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[1], label_batch[1]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized review\", integer_vectorize_text(first_review, first_label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTjpy6Y4gbxk"
      },
      "source": [
        "This review was shorter than our set output length of 250 tokens, and is padded with index 0, which we mapped to an empty string (''). That makes sense, since there are literally no more words past the end of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1P6NembgmpP"
      },
      "source": [
        "For yet aother test, we can do both the encoding and decoding side-by-side:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjOTn6PjgodI",
        "outputId": "da55631e-7ef6-4bfb-c172-ae40d1419652"
      },
      "outputs": [],
      "source": [
        "print('original  encoded    decoded')\n",
        "\n",
        "\n",
        "first_review_vectorised = integer_vectorize_text(first_review, first_label)\n",
        "for i in range(20):\n",
        "    words = tf.strings.split(first_review)[i].numpy()\n",
        "    vect = first_review_vectorised[0][0][i].numpy()\n",
        "    outword = integer_vectorize_layer.get_vocabulary()[vect]\n",
        "\n",
        "    print('{:10} {:5}     {:10}'.format(words.decode(), vect, outword))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDWw82sggvvv"
      },
      "source": [
        "Finally, when we are happy with the vectorisation, apply it to all the TensorFlow datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYEBvSv6gxVw"
      },
      "outputs": [],
      "source": [
        "integer_train_ds = raw_train_ds.map(integer_vectorize_text)\n",
        "integer_val_ds = raw_val_ds.map(integer_vectorize_text)\n",
        "integer_test_ds = raw_test_ds.map(integer_vectorize_text)\n",
        "\n",
        "integer_train_ds = integer_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "integer_val_ds = integer_val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "integer_test_ds = integer_test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb69choAg8Uj"
      },
      "source": [
        "### Train a convolutional model\n",
        "\n",
        "We already know two types of models that can process sequence data, the first one being the convolutional network.\n",
        "\n",
        "### <span style=\"color: red;\">Exercise:<span>\n",
        "\n",
        "Implement a CNN below, and test it. We have alredy added a `Lambda` layer to make sure the data shape matches what a `Conv1D` layers expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "Rp59Q-pYhRtA",
        "outputId": "8d3c0e5c-9d54-4ffd-8ddb-9ada949244ba"
      },
      "outputs": [],
      "source": [
        "cnn_model = keras.Sequential([\n",
        "    keras.Input(shape=(sequence_length,)),\n",
        "    keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1)),\n",
        "\n",
        "    # Your code\n",
        "    # ...\n",
        "\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "cnn_model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['binary_accuracy']\n",
        ")\n",
        "cnn_model.summary()\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "history = cnn_model.fit(\n",
        "    integer_train_ds,\n",
        "    validation_data=integer_val_ds,\n",
        "    epochs=epochs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND5HbrPymQor"
      },
      "source": [
        "### Train an RNN model\n",
        "\n",
        "Next, give it a go with a recurrent network, for instance bidirectional LSTM.\n",
        "\n",
        "In case you don't want to wait out the training, you don't have to :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9egyefFQmMU_"
      },
      "outputs": [],
      "source": [
        "lstm_model = keras.Sequential([\n",
        "    keras.Input(shape=(sequence_length,)),\n",
        "    keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1)),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(32)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "lstm_model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['binary_accuracy']\n",
        ")\n",
        "lstm_model.summary()\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "history = lstm_model.fit(\n",
        "    integer_train_ds,\n",
        "    validation_data=integer_val_ds,\n",
        "    epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkA2lGpAqHms"
      },
      "source": [
        "## Improving learning through word embeddings\n",
        "\n",
        "In case you are underwhelmed by the performance of the CNN and LSTM models, there is luckily a trick that will help us: _Word embeddings_.\n",
        "\n",
        "This is mainly the topic for the next notebook, but you can try it already here, to see if if improves our LSTM model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHl0xgCloL4A"
      },
      "outputs": [],
      "source": [
        "lstm_model = keras.Sequential([\n",
        "    keras.Input(shape=(sequence_length,)),\n",
        "    keras.layers.Embedding(input_dim=max_features, output_dim=256),     # hmm!\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(32)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "lstm_model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['binary_accuracy']\n",
        ")\n",
        "lstm_model.summary()\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "history = lstm_model.fit(\n",
        "    integer_train_ds,\n",
        "    validation_data=integer_val_ds,\n",
        "    epochs=epochs\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
