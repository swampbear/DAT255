{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v8KgpxMRcpl"
      },
      "source": [
        "# Word embeddings\n",
        "\n",
        "_Word embeddings_ is a trick that significantly improves the performance of NLP models, and all modern LLMs rely on this. In this notebook we will try to get an understanding of how they work.\n",
        "\n",
        "Like for the last notebook, let us use the IMDb movie review dataset to do sentiment analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6auAVGsKVJ9J"
      },
      "source": [
        "Load TensorBoard, which we will use for visualisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4CCV8UaCEVZ"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0G5k-ETVSvJ"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7qk_C5qCTtl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import string\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets\n",
        "from tensorboard.plugins import projector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17aSZ9WSVVfd"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFqhXZvVrf4d"
      },
      "source": [
        "For an easier data loading process, we can download the data directly into TensorFlow using the [TensorFlow datasets](https://www.tensorflow.org/datasets) extension.\n",
        "\n",
        "You can find several intereseting datasets readily available here, at the expense of being somewhat cumbersome to look at and understand, since they are already `Tensor`s. Anyway, here goes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cTKCO30sFf3"
      },
      "outputs": [],
      "source": [
        "dataset, info = tensorflow_datasets.load(\n",
        "    'imdb_reviews',\n",
        "    with_info=True,\n",
        "    as_supervised=True\n",
        ")\n",
        "train_ds, test_ds = dataset['train'], dataset['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCKJfWEbsSCf"
      },
      "source": [
        "Look at the first review:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DatrVEpsUHt"
      },
      "outputs": [],
      "source": [
        "for example, label in train_ds.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVdq1tpQsePu"
      },
      "source": [
        "## Text vectorisation\n",
        "\n",
        "Like before, we remove punctuation, split words on whitespace, and remove any pesky HTML tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUFg_YfIs7ek"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  without_html = tf.strings.regex_replace(lowercase, '<[^>]*>', ' ')\n",
        "  without_punctuation = tf.strings.regex_replace(without_html, '[{}]'.format(string.punctuation), '')\n",
        "  return without_punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9epk7wfbtD_F"
      },
      "source": [
        "Instantiate and adapt the `TextVectorization` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o_Fd0rptY6a"
      },
      "outputs": [],
      "source": [
        "max_features = 10000\n",
        "sequence_length = 300   # cut the text if longer than this\n",
        "\n",
        "vectorize_layer = keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "vectorize_layer.adapt(\n",
        "    train_ds.map(lambda x, y: x)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkiLiIPttfTl"
      },
      "source": [
        "...and apply to the TensorFlow datasets.\n",
        "\n",
        "In this case we also need to expand the dimensions of both the data and the label, which is an annoyance you'll get used to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCq6iZKsyrub"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "  label = tf.expand_dims(label, -1)\n",
        "  #text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnlqsmvitjOs"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: vectorize_text(x, y))\n",
        "test_ds = test_ds.map(lambda x, y: vectorize_text(x, y))\n",
        "\n",
        "train_ds = train_ds.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNa5D9ETd9y3"
      },
      "source": [
        "We can look at the vocabulary by calling `get_vocabulary()`. Print the ten first entries, along with their indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKjG6BwsljKx"
      },
      "outputs": [],
      "source": [
        "ten_first = vectorize_layer.get_vocabulary()[:10]\n",
        "\n",
        "print('index    token')\n",
        "for i, v in enumerate(ten_first):\n",
        "    print(f'{i} \\t \\'{v}\\'')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faDx2HWvhKHV"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "The crucial part of our model will be the `Embedding` layer, which encodes the token indices into a vector of floating-point values. We are free to define the dimensions of the embedding ourselves.\n",
        "\n",
        "However, as explained in the textbook on page 471, using an embedding dimension that is larger than the number of units in the preceeding layer, is not very useful.\n",
        "\n",
        "For the rest of the model, we keep it simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuSUaMTnTaB4"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 32\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    keras.Input(shape=(sequence_length,)),\n",
        "    keras.layers.Embedding(max_features, embedding_dim),\n",
        "    keras.layers.GlobalAveragePooling1D(),\n",
        "    keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.L2(0.01)),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxbTtIbkTf7E"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['binary_accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jsnbEuOhRKP"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e31ocXEZu2c_"
      },
      "source": [
        "Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1XHH_FKTjh9"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkjCgwa-UMmj"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci8moChrvHu3"
      },
      "source": [
        "If we like, we can write some reviews ourselves and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R36XKXGyUWCg"
      },
      "outputs": [],
      "source": [
        "model_with_vectorisation = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "])\n",
        "\n",
        "model_with_vectorisation.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['binary_accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezp8E4YqUd__"
      },
      "outputs": [],
      "source": [
        "examples = tf.constant([\n",
        "  \"It was the best movie in the history of movies, maybe ever. \"\n",
        "])\n",
        "\n",
        "pred = model_with_vectorisation.predict(examples)\n",
        "print('Review was {:.3f}% positive.'.format(pred[0][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYecRs3jv-cY"
      },
      "source": [
        "## Visualise the embedding space\n",
        "\n",
        "To make any sence of the positions of the different words in embedding space, let's plot it using TensorBoard.\n",
        "\n",
        "The code below writes two pieces of information to files that TensorBoard can read:\n",
        "- The list of words in the vocabulary (goes in `metadata.tsv`)\n",
        "- The weights of the embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2p7jt0aVMW7"
      },
      "outputs": [],
      "source": [
        "log_dir='/logs/imdb/'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# Write one vocabulary entry per line\n",
        "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
        "\n",
        "  for subwords in vectorize_layer.get_vocabulary():\n",
        "    f.write(\"{}\\n\".format(subwords))\n",
        "\n",
        "# Save the weights we want to analyze as a variable. Note that the first\n",
        "# value represents \"no word\", which we remove.\n",
        "weights = tf.Variable(model.layers[0].get_weights()[0][1:])\n",
        "\n",
        "# Create a checkpoint from embedding, the filename and key are the\n",
        "# name of the tensor\n",
        "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
        "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
        "\n",
        "# Set up config.\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "\n",
        "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
        "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
        "embedding.metadata_path = 'metadata.tsv'\n",
        "projector.visualize_embeddings(log_dir, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c12izsMkxKVB"
      },
      "source": [
        "Check the metadata file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUvg254iWvEa"
      },
      "outputs": [],
      "source": [
        "! wc -l /logs/imdb/metadata.tsv\n",
        "!head /logs/imdb/metadata.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYRNRFZcxSi7"
      },
      "source": [
        "### Run TensorBoard\n",
        "\n",
        "To show the embeddings, click the dropdown menu in the upper right and select PROJECTOR.\n",
        "\n",
        "Now you can hover over the different words, and click them to find neighbouring words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV6GKqWgC8OP"
      },
      "source": [
        "### <span style=\"color: red;\">Exercise:<span>\n",
        "\n",
        "Investigate the embeddings and see if you find that the neighbours are in fact related, and have a meaningful position in the embedding space.\n",
        "\n",
        "Remember that our embedding space was 32-dimensional, which is hard to visualise since we are limited to living in 3-dimensional space. TensorBoard will help us by projecting everything down to 3 dimensions, which means that for the plot we see, a lot of information is lost. Use the distance measures on the right to guide you when comparing words.\n",
        "\n",
        "If you train for longer or create a better performing model, how does the embedding space change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EszJbAwVdCt"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /logs/imdb/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
