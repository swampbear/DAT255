{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALk8F_w4W-iF"
      },
      "source": [
        "## Embeddings, and more modern networks for tabular data\n",
        "\n",
        "One-hot encoding of categorical data is nice and effective, but let's try the embedding trick. We can run a standard dense network on top of the embeddings, but we also want to give it a go with _transformers_, in which case the embeddings are necessary.\n",
        "\n",
        "We will split our feature is two types: numerical, which are normalised, and categorical, which are converted into embeddings.\n",
        "\n",
        "Implementing the [TabTransformer](https://arxiv.org/abs/2012.06678) will be a difficult exercise, but mostly difficult in terms of sending the different features to the right parts of the network. Composing the network itself, using Keras components, is still quite convenient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNCSJK11ZKbW"
      },
      "source": [
        "For out data, we will try to classify bank customers with good or bad credit risk. The dataset is described at the UCI ML dataset [repository](https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data).\n",
        "\n",
        "Download and unzip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuW64Y1sUUBn"
      },
      "outputs": [],
      "source": [
        "!wget https://archive.ics.uci.edu/static/public/144/statlog+german+credit+data.zip\n",
        "!unzip -u statlog+german+credit+data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hUbc0eaZrIK"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgnB_LZFYKIQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thqJgO3HZuEm"
      },
      "source": [
        "## Define feature types\n",
        "\n",
        "The ones that are `\"Categorical\"` will go into the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yZSGHHjZTvY"
      },
      "outputs": [],
      "source": [
        "feature_types = {\n",
        "    \"Existing checking account\": \"Categorical\",\n",
        "    \"Duration\": \"Integer\",\n",
        "    \"Credit history\": \"Categorical\",\n",
        "    \"Purpose\":\"Categorical\",\n",
        "    \"Credit amount\": \"Integer\",\n",
        "    \"Savings account\": \"Categorical\",\n",
        "    \"Employment since\": \"Categorical\",\n",
        "    \"Installment rate\": \"Integer\",\n",
        "    \"Personal status\": \"Categorical\",\n",
        "    \"Other debtors\": \"Categorical\",\n",
        "    \"Present residence since\": \"Integer\",\n",
        "    \"Property\": \"Categorical\",\n",
        "    \"Age\": \"Integer\",\n",
        "    \"Other installment plans\": \"Categorical\",\n",
        "    \"Housing\": \"Categorical\",\n",
        "    \"Existing credits\": \"Integer\",\n",
        "    \"Occupation\": \"Categorical\",\n",
        "    \"Maintenance\": \"Integer\",\n",
        "    \"Telephone\": \"Categorical\",\n",
        "    \"Foreign worker\": \"Categorical\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5rDHfaeawvO"
      },
      "source": [
        "Read the CSV file, for instance using Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq16cVrk4bKe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dataframe = pd.read_csv(\"german.data\", header=None, sep='\\s+', names=list(feature_types.keys())+[\"Target\"], index_col=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GuOVtB6-IkW"
      },
      "outputs": [],
      "source": [
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ5fhab2a0YZ"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "We see that all categorical features are in fact strings with some weird string encoding. We need numbers to work with, so let's convert all to ordinal, integer encodings. We can use scikit-learn's `OrdinalEncoder`for this.\n",
        "\n",
        "We do **not** convert to one-hot encodings in this case, because we will proceed with making embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPU9-1aV66_G"
      },
      "outputs": [],
      "source": [
        "# Get the list of which features are categorical\n",
        "categorical_features = [fname for fname, ftype in feature_types.items() if ftype == \"Categorical\"]\n",
        "\n",
        "# Apply ordinal encoding for these\n",
        "dataframe[categorical_features] = OrdinalEncoder().fit_transform(dataframe[categorical_features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYpg_aFTbjKN"
      },
      "source": [
        "Let's see if it looks right:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eoL4h8P61yD"
      },
      "outputs": [],
      "source": [
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn4XFW9Dbl4T"
      },
      "source": [
        "Great. Only one strange thing -- the targets are 1 and 2, not 0 and 1. Fix this by subtracting one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQqV5edP-__P"
      },
      "outputs": [],
      "source": [
        "labels = dataframe.pop(\"Target\")\n",
        "labels = labels - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6FCnjcBdWif"
      },
      "source": [
        "Create a train-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LST6yVEbfMkL"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dataframe, labels, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jM1Ka8QdjWA"
      },
      "source": [
        "Create TensorFlow datasets, and batch them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL506uPudvt0"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((dict(X_test), y_test))\n",
        "\n",
        "batch_size = 32\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "test_ds = test_ds.batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VSHlZ7UcyFH"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "We start by defining a function to prepare the inputs to our model -- just adding `Input` layers with the correct data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gak_hlqvdlM8"
      },
      "outputs": [],
      "source": [
        "def prepare_inputs():\n",
        "\n",
        "    inputs = {}\n",
        "\n",
        "    for feature_name, feature_type in feature_types.items():\n",
        "\n",
        "        # Choose data type\n",
        "        if feature_type in [\"Binary\", \"Categorical\"]:\n",
        "            dtype = \"int32\"\n",
        "        else:\n",
        "            dtype = \"float32\"\n",
        "\n",
        "        input_layer = keras.layers.Input(name=feature_name, shape=(1,), dtype=dtype)\n",
        "\n",
        "        inputs[feature_name] = input_layer\n",
        "\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6qHRaBGqveo"
      },
      "source": [
        "Now for the important part:\n",
        "\n",
        "Do normalisation of numerical features, and create embeddings for the categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpfIn7pUqAwx"
      },
      "outputs": [],
      "source": [
        "def process_inputs(inputs, embedding_dims):\n",
        "\n",
        "    processed_inputs = {}\n",
        "\n",
        "    for feature_name, input_layer in inputs.items():\n",
        "\n",
        "        # Binary features: leave as they are\n",
        "        if feature_types[feature_name] == \"Binary\":\n",
        "            processed_inputs[feature_name] = input_layer\n",
        "\n",
        "        # Numeric features: Apply normalisation\n",
        "        elif feature_types[feature_name] == \"Integer\":\n",
        "\n",
        "            norm_layer = keras.layers.Normalization(axis=None)\n",
        "            norm_layer.adapt(X_train[feature_name].to_numpy())\n",
        "\n",
        "            processed_inputs[feature_name] = norm_layer(input_layer)\n",
        "\n",
        "\n",
        "        # Categorical features: Create embeddings\n",
        "        elif feature_types[feature_name] == \"Categorical\":\n",
        "\n",
        "            # Check how many categories we have\n",
        "            num_categories = len(np.unique(X_train[feature_name]))\n",
        "\n",
        "            # Add the embedding layer\n",
        "            embedding_layer = keras.layers.Embedding(\n",
        "                input_dim=num_categories,\n",
        "                output_dim=embedding_dims\n",
        "            )\n",
        "\n",
        "            processed_inputs[feature_name] = keras.layers.Flatten()(embedding_layer(input_layer))\n",
        "\n",
        "    return processed_inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcoYcxQIkkvv"
      },
      "source": [
        "For our first model -- Create a simple dense network.\n",
        "\n",
        "At this point we have to select the dimensions for the embedding layers, which something that have to be optimised by testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgSgIZzQkkeA"
      },
      "outputs": [],
      "source": [
        "def create_simple_model():\n",
        "\n",
        "    embedding_size = 16\n",
        "\n",
        "    inputs = prepare_inputs()\n",
        "    processed_inputs = process_inputs(inputs, embedding_size)\n",
        "    all_inputs = keras.layers.concatenate(list(processed_inputs.values()))\n",
        "\n",
        "    x = keras.layers.Dense(64, activation=\"relu\", name=\"dense_1\")(all_inputs)\n",
        "    out = keras.layers.Dense(1, activation=\"sigmoid\", name=\"dense_2\")(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=out)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0-HUFABep54"
      },
      "source": [
        "Instantiate the model, compile it, and have a look at the structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRwR9ayOlpo_"
      },
      "outputs": [],
      "source": [
        "simple_model = create_simple_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykXa_IeXtYB8"
      },
      "outputs": [],
      "source": [
        "simple_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU8XcvUntDVT"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(simple_model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnhAn8lwtJRA"
      },
      "source": [
        "Train it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibbuWtGltKqK"
      },
      "outputs": [],
      "source": [
        "simple_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksvLXAjgfJqT"
      },
      "source": [
        "## Implement the TabTransformer\n",
        "\n",
        "Let's take a stab at the [TabTransformer](https://arxiv.org/pdf/2012.06678), which looks like this:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/keras-team/keras-io/master/examples/structured_data/img/tabtransformer/tabtransformer.png\" width=\"450\"/>\n",
        "\n",
        "We see that the categorical features goes into embeddings, which we have done already, while the numerical features are normalised, which we also did. The remaing part is to add the layers of the network and connect the parts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tk-nXWKiPVP"
      },
      "source": [
        "### <span style=\"color: red; font-weight: bold;\">Exercise (difficult):<span>\n",
        "\n",
        "Complete the TabTransformer model.\n",
        "\n",
        "Some hints:\n",
        "- You will have to collect the embedded features and the numerical features separately. In the simple model above we collected everything together like using `all_inputs = keras.layers.concatenate(list(processed_inputs.values()))`, so this has to be split into two.\n",
        "- The basic structure of the transformer block is given below.\n",
        "- You will need to use `keras.layers.concatenate` to merge the outputs from the transformer block with the numerical features.\n",
        "- The network should end with `layers.Dense(units=1, activation=\"sigmoid\")`.\n",
        "\n",
        "Good luck!\n",
        "\n",
        "For more hints, you can look at the relevant Keras [example](https://keras.io/examples/structured_data/tabtransformer/), which this notebook is based on. We use different data, but the idea is the same. The example does write the code in a more convoluted way, but most parts is similar to here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2VE-UXBfU9z"
      },
      "outputs": [],
      "source": [
        "def transformer_block(categorical_features):\n",
        "\n",
        "    # Self-attention: Call it two inputs, which are the same.\n",
        "    attention_output = keras.layers.MultiHeadAttention(\n",
        "        num_heads=2,\n",
        "        key_dim=embedding_size,\n",
        "        dropout=0.2\n",
        "    )(categorical_features, categorical_features)\n",
        "\n",
        "    # Skip connection 1\n",
        "    x = keras.layers.Add()([attention_output, categorical_features])\n",
        "\n",
        "    # Layer normalization 1\n",
        "    x = keras.layers.LayerNormalization()(x)\n",
        "\n",
        "    # Feedforward\n",
        "    feedforward_output = keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "    feedforward_output = keras.layers.Dropout(0.2)(feedforward_output)\n",
        "\n",
        "    # Skip connection 2.\n",
        "    x = keras.layers.Add()([feedforward_output, x])\n",
        "\n",
        "    # Layer normalization 2.\n",
        "    output = keras.layers.LayerNormalization()(x)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2tSExQvlymd"
      },
      "outputs": [],
      "source": [
        "def create_tabtransformer():\n",
        "\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttZDRkJWlrhU"
      },
      "source": [
        "Train your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIfc6a7CluAr"
      },
      "outputs": [],
      "source": [
        "tabtransformer.fit(\n",
        "    train_ds,\n",
        "    ...\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
