{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO_2aOutv_nV"
      },
      "source": [
        "# Anomaly detection in time series\n",
        "\n",
        "In this notebook we will train an encoder-decoder model that learns to reconstruct its own inputs -- an autoencoder. We already investigated this type of model in notebook 13 where we used it for denoising images, but now we turn to a different type of task, which is to look for unexpected changes in data, so-called anomalies.\n",
        "\n",
        "Again we base ourselves on a Keras example, but give it a try yourself before checking the solution on the Keras webpage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b2ptC3iv_nZ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwwASvNVv_nZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em80Ywtkv_nb"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "The [Numenta Anomaly Benchmark (NAB)](\n",
        "https://www.kaggle.com/boltzmannbrain/nab) dataset contains simulated time series data, labelled in sections of normal and anomalous behaviour. There are two files: `art_daily_small_noise.csv`, which contains normal data we will use for training, and `art_daily_jumpsup.csv` which we use for testing.\n",
        "\n",
        "For simplicity we read the CSV files with the Pandas library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc6U_blDv_nc"
      },
      "outputs": [],
      "source": [
        "master_url_root = \"https://raw.githubusercontent.com/numenta/NAB/master/data/\"\n",
        "\n",
        "df_small_noise_url_suffix = \"artificialNoAnomaly/art_daily_small_noise.csv\"\n",
        "df_small_noise_url = master_url_root + df_small_noise_url_suffix\n",
        "df_small_noise = pd.read_csv(\n",
        "    df_small_noise_url, parse_dates=True, index_col=\"timestamp\"\n",
        ")\n",
        "\n",
        "df_daily_jumpsup_url_suffix = \"artificialWithAnomaly/art_daily_jumpsup.csv\"\n",
        "df_daily_jumpsup_url = master_url_root + df_daily_jumpsup_url_suffix\n",
        "df_daily_jumpsup = pd.read_csv(\n",
        "    df_daily_jumpsup_url, parse_dates=True, index_col=\"timestamp\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FengHfm-v_nd"
      },
      "source": [
        "Print data contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfFrY6m_v_ne"
      },
      "outputs": [],
      "source": [
        "print(df_small_noise.head())\n",
        "\n",
        "print(df_daily_jumpsup.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1PEzIBcv_nf"
      },
      "source": [
        "## Visualise the data\n",
        "\n",
        "The training data looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3ZO0jcKv_ng"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "df_small_noise.plot(legend=False, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvYiHxaFv_nh"
      },
      "source": [
        "While our test data contains an unexpected jump, that we should hopefully be able to detect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMMclzvxv_ni"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "df_daily_jumpsup.plot(legend=False, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9BZ19M-v_nj"
      },
      "source": [
        "## Prepare training data\n",
        "\n",
        "Get data values from the training timeseries data file and normalize the\n",
        "`value` data. We have a `value` for every 5 mins for 14 days.\n",
        "\n",
        "-   24 * 60 / 5 = **288 timesteps per day**\n",
        "-   288 * 14 = **4032 data points** in total\n",
        "\n",
        "As usual, we normalise by subtracting the mean and dividing by the standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkltJHM1v_nk"
      },
      "outputs": [],
      "source": [
        "training_mean = df_small_noise.mean()\n",
        "training_std = df_small_noise.std()\n",
        "df_training_value = (df_small_noise - training_mean) / training_std\n",
        "print(\"Number of training samples:\", len(df_training_value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0lZTEhbv_nk"
      },
      "source": [
        "### Create fixed-length sequences of data\n",
        "\n",
        "Create sequences combining `TIME_STEPS` contiguous data values from the\n",
        "training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpXOZy3Yv_nl"
      },
      "outputs": [],
      "source": [
        "TIME_STEPS = 288\n",
        "\n",
        "# Generated training sequences for use in the model.\n",
        "def create_sequences(values, time_steps=TIME_STEPS):\n",
        "    output = []\n",
        "    for i in range(len(values) - time_steps + 1):\n",
        "        output.append(values[i : (i + time_steps)])\n",
        "    return np.stack(output)\n",
        "\n",
        "\n",
        "x_train = create_sequences(df_training_value.values)\n",
        "print(\"Training input shape: \", x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhffX2Upv_nl"
      },
      "source": [
        "### <span style=\"color: red; font-weight: bold;\">Exercise: Build the autoencoder<span>\n",
        "\n",
        "This time you are left on your own to create the model! But some hints to create a baseline architecture that should perform reasonably well:\n",
        "\n",
        "- Two `Conv2D` layers with `kernel_size` around 5-7\n",
        "- Two `Conv2DTranspose` layers to get back to the inital input shape\n",
        "- Use either `strides=2` or pooling layers to downsample the data into our \"compressed\" or \"encoded\" representation\n",
        "- Some dropout is probably nice.\n",
        "- The output is a time series with identical shap to the input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN1MJTqdv_nm"
      },
      "outputs": [],
      "source": [
        "model = ...\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p-lnpH4v_nn"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "For our autoencoder the target is the input, so we need to specify this correctly in the `fit` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PCMG2Wqv_nn"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    x_train,\n",
        "    x_train,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCubulxGv_no"
      },
      "source": [
        "Plot training and validation loss to evaluate the training procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzD10JQ0v_np"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns7p5983v_np"
      },
      "source": [
        "## Detecting anomalies\n",
        "\n",
        "We will try to detect anomalies by determining how well our model is able to reconstruct the input data.\n",
        "\n",
        "Let's use the mean absolute error (MAE) as our metric: the absolute difference between each data point and the prediction, averaged over all data points in the time series.\n",
        "\n",
        "To classify anomalies, we need to set a threshold for how high a MAE value we consider as anomalous. We can select the threshold for instance by computing the MAE for all the sequences in the training data, and set the threshold equal to the highest (=worst) value we see. Anything above this value, we then consider to be an anomaly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBsAsLjAv_nq"
      },
      "outputs": [],
      "source": [
        "# Predict for all test sequences\n",
        "x_train_pred = model.predict(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLx8_9ia23UB"
      },
      "source": [
        "### <span style=\"color: red; font-weight: bold;\">Exercise:<span>\n",
        "\n",
        "Compute the mean average errors -- either by writing the function yourself, or using `keras.metrics.MeanAbsoluteError`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs1JgGsv22wO"
      },
      "outputs": [],
      "source": [
        "mean_average_errors = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifYPePe74NYx"
      },
      "source": [
        "Then we take the maximum as out threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taGfWnVP4KRD"
      },
      "outputs": [],
      "source": [
        "threshold = np.max(mean_average_errors)\n",
        "print(\"Reconstruction error threshold: \", threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3L86nL9v_nr"
      },
      "source": [
        "## Compare recontruction\n",
        "\n",
        "Before we start looking for anomalies, let's see how our model has recontructed the first sample. This is the 288 timesteps from day 1 of our training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p53duHUPv_ns"
      },
      "outputs": [],
      "source": [
        "plt.plot(x_train[0])\n",
        "plt.plot(x_train_pred[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cPaACHuv_ns"
      },
      "source": [
        "## Prepare test data\n",
        "\n",
        "Normalise out test data and create sequences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pygfnp-xv_ns"
      },
      "outputs": [],
      "source": [
        "df_test_value = (df_daily_jumpsup - training_mean) / training_std\n",
        "fig, ax = plt.subplots()\n",
        "df_test_value.plot(legend=False, ax=ax)\n",
        "plt.show()\n",
        "\n",
        "# Create sequences from test values.\n",
        "x_test = create_sequences(df_test_value.values)\n",
        "print(\"Test input shape: \", x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzDcmLl-4mTs"
      },
      "source": [
        "## Find anomalies\n",
        "\n",
        "Now for the real test: Compute MAE for all sequences in the test set, and check if any break the threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEjPlA5F4o9D"
      },
      "outputs": [],
      "source": [
        "x_test_pred = model.predict(x_test)\n",
        "test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n",
        "test_mae_loss = test_mae_loss.reshape((-1))\n",
        "\n",
        "plt.hist(test_mae_loss, bins=50)\n",
        "plt.xlabel(\"test MAE loss\")\n",
        "plt.ylabel(\"No of samples\")\n",
        "plt.show()\n",
        "\n",
        "# Detect all the samples which are anomalies.\n",
        "anomalies = test_mae_loss > threshold\n",
        "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
        "print(\"Indices of anomaly samples: \", np.where(anomalies))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeenOPZVv_nt"
      },
      "source": [
        "## Plot anomalies\n",
        "\n",
        "We now know the samples of the data which are anomalies. With this, we will\n",
        "find the corresponding `timestamps` from the original test data. We will be\n",
        "using the following method to do that:\n",
        "\n",
        "Let's say time_steps = 3 and we have 10 training values. Our `x_train` will\n",
        "look like this:\n",
        "\n",
        "- 0, 1, 2\n",
        "- 1, 2, 3\n",
        "- 2, 3, 4\n",
        "- 3, 4, 5\n",
        "- 4, 5, 6\n",
        "- 5, 6, 7\n",
        "- 6, 7, 8\n",
        "- 7, 8, 9\n",
        "\n",
        "All except the initial and the final time_steps-1 data values, will appear in\n",
        "`time_steps` number of samples. So, if we know that the samples\n",
        "[(3, 4, 5), (4, 5, 6), (5, 6, 7)] are anomalies, we can say that the data point\n",
        "5 is an anomaly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_Nt3ja1v_nt"
      },
      "outputs": [],
      "source": [
        "# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\n",
        "anomalous_data_indices = []\n",
        "for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):\n",
        "    if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):\n",
        "        anomalous_data_indices.append(data_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIvJ92Zmv_nu"
      },
      "source": [
        "Overlay the anomalies on the original test data plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UjVqep0v_nu"
      },
      "outputs": [],
      "source": [
        "df_subset = df_daily_jumpsup.iloc[anomalous_data_indices]\n",
        "fig, ax = plt.subplots()\n",
        "df_daily_jumpsup.plot(legend=False, ax=ax)\n",
        "df_subset.plot(legend=False, ax=ax, color=\"r\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
