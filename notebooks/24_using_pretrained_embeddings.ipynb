{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojr2yWAN5lHb"
      },
      "source": [
        "# Using pretrained embeddings\n",
        "\n",
        "Instead of training our own embedding layers, we can download and use pre-trained ones, much like we could use pretrained computer vision models and fine-tune them, without re-training the first feature-extraction layers. This is particularly useful in cases where we have little training data.\n",
        "\n",
        "In this case we will use the [GloVe](https://nlp.stanford.edu/projects/glove/) (Global Vectors for Word Representation) embeddings, which are orignally trained in an unsupervised setting. These are from before big LLMs were a thing, but still capture semantic similarity very well. The GloVe embeddings come in different dimensionalities: 50-, 100-, 200- and 300-dimensional.\n",
        "\n",
        "Modern LLM embeddings are often a lot bigger, typically with 3000-4000 dimensions. The quality of these are listed on the Huggingface [embedding leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n",
        "For serious use cases one would choose from these instead, but for our simple example, GloVe will do fine.\n",
        "\n",
        "Let's again classify IMDb film reviews.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVAtWeVz4Knd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUCi7eYh9yO2"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "This part is the same as for notebook 22."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d81jIFGD-Ty0"
      },
      "outputs": [],
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = keras.utils.get_file(\n",
        "    \"aclImdb_v1\",\n",
        "    url,\n",
        "    untar=True,\n",
        "    cache_dir='.',\n",
        "    cache_subdir=''\n",
        ")\n",
        "\n",
        "dataset_dir = 'aclImdb_v1/aclImdb'\n",
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)\n",
        "\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the TensorFlow datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvMnIy0f9Kzb"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    os.path.join(dataset_dir, 'train'),\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    os.path.join(dataset_dir, 'train'),\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    os.path.join(dataset_dir, 'test'),\n",
        "    batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our nice text standardisation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOSDm5tm9aQf"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  without_html = tf.strings.regex_replace(lowercase, '<[^>]*>', ' ')\n",
        "  without_punctuation = tf.strings.regex_replace(without_html, '[{}]'.format(string.punctuation), '')\n",
        "  return without_punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQ4Awwtb9e-C"
      },
      "outputs": [],
      "source": [
        "max_features = 20000\n",
        "sequence_length = 300   # cut the text if longer than this\n",
        "\n",
        "vectorize_layer = keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "vectorize_layer.adapt(train_ds.map(lambda x, y: x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply vectorisation, and cache / prefetch data for performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hoOpNLiAwfy"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.map(lambda x, y: (vectorize_layer(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (vectorize_layer(x), y))\n",
        "test_ds = test_ds.map(lambda x, y: (vectorize_layer(x), y))\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNRhlGy2_zWi"
      },
      "source": [
        "## Download pretrained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeZOpcOuALgJ"
      },
      "source": [
        "The GloVe embeddings come in a 822MB zip file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W14sqsYC_yzj"
      },
      "outputs": [],
      "source": [
        "!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ru0y4VA9JA"
      },
      "source": [
        "We will be using the 100-dimensional embeddings. Let's have a look at the file (but truncate the lines since they are very long):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z729tmFrAn7v"
      },
      "outputs": [],
      "source": [
        "! head -n 20 glove.6B.100d.txt | cut -c 1-100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbeiuOFjBPSG"
      },
      "source": [
        "The file consists of space-separated entries, where the first column is the token, and the remaing hundred are the embedding axes.\n",
        "\n",
        "We read the entire thing into a dictionary, mapping each word to its embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDBJtD78ACQC"
      },
      "outputs": [],
      "source": [
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzmAjQ-0CCEG"
      },
      "source": [
        "## Prepare the embeddings\n",
        "\n",
        "Now we need to make it match the word list, or rather the word (token) indices, that we have in our data of IMDb film reviews.\n",
        "\n",
        "Our `Embedding` layer consist of a matrix where the row number `i` contains the embedding vector for token number `i`.\n",
        "\n",
        "We loop through our entire vocabulary, and set the corresponding embedding vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKTLUv77CBrV"
      },
      "outputs": [],
      "source": [
        "vocabulary = vectorize_layer.get_vocabulary()\n",
        "\n",
        "num_tokens = len(vocabulary) #  + 2    # +2 for \"padding\" and \"OOV\"\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Initialise matrix to zeros.\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "\n",
        "# Loop over known words\n",
        "for i, word in enumerate(vocabulary):\n",
        "\n",
        "    # Get embedding vector\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "\n",
        "    # Copy it to the matrix\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "\n",
        "    # Not found? Leave it as zeros.\n",
        "    # This includes the representation for \"padding\" and \"OOV\"\n",
        "    else:\n",
        "        misses += 1\n",
        "\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqtHjnMqEdRJ"
      },
      "source": [
        "Great, now we apply this matrix to an `Embedding` layer.\n",
        "\n",
        "We set `trainable=False`, meaning the embeddings will not be updated during model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1c9NXatEliX"
      },
      "outputs": [],
      "source": [
        "embedding_layer = keras.layers.Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    trainable=False,\n",
        ")\n",
        "embedding_layer.build((1,))\n",
        "embedding_layer.set_weights([embedding_matrix])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0NL9SUbE1LZ"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "### <span style=\"color: red;\">Exercise:<span>\n",
        "\n",
        "Build a useful model that takes the embedding layer as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeSqU_wkE0ap"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wikHCNAFPLy"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoBhbmr7BSOQ"
      },
      "outputs": [],
      "source": [
        "for batch in train_ds.take(1):\n",
        "    print(batch[0].shape)\n",
        "    print(batch[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A_JB2f3FUIb"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"binary_accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNgmkgYDFnTU"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDqgVOi3Bztn"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print('Test set accuracy:', accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
